# -*- encoding:utf-8 -*-
import json

from audio_analysis.api import audio_to_text
from text_analysis.utils import sen_generator, preprocess_audio_text, segment_split, sentence_split
from text_analysis.tencent import TENCENT_API


def generate_abstract_from_audio(file: str) -> str:
    """
    根据音频文件，先转换为文本，然后进行文本摘要
    :param file:
    :return:
    """
    ret = {
        'AText': '',  # 语音转文本结果
        'TAbstract': None,  # 文本摘要
        'Error': ''  # 错误信息
    }
    try:
        text = preprocess_audio_text(audio_to_text(file))
    except Exception as e:
        print(e)
        ret['Error'] = 'failed to transfer audio to text'
    else:
        try:
            if text != '':
                ret['AText'] = text
                ret['TAbstract'] = text_summarize(text)
        except Exception as e:
            print(e)
            ret['Error'] = 'failed to summarize the text'

    return json.dumps(ret, ensure_ascii=False)


def text_summarize(text: str) -> [str]:
    """
    文本摘要
    :param text: 经过预处理后的文本
    :return: 摘要列表，按照语义进行了切分，每个列表元素是一个段落
    """
    ret = []
    for seg in sen_generator(text):
        try:
            # 文本纠错
            tmp = TENCENT_API.text_correction(seg)
            # 每次取出长度不超过2000字符的段落进行摘要
            ret.append(TENCENT_API.auto_summarization(tmp))
        except Exception as e:
            print('in text_summarize: ', e)
    return segment_split(sentence_split(''.join(ret)))


def cal_text_similarity(text_a, text_b: str) -> float:
    """
    计算两个文本相似度
    :param text_a: 文本1
    :param text_b: 文本2
    :return: 相似度[0, 1]，数值越大越相似
    """
    return TENCENT_API.text_similarity(text_a, text_b)

# if __name__ == '__main__':
#     beg = time.time()
#     # print(generate_abstract_from_audio('/swc/resource/1617010399/MTYxNzAxMDQxMC4yNDM2OTJodHRwczovL3d3dy5iaWxpYmlsaS5jb20vdmlkZW8vQlYxM1o0eTFHN1Ax.mp3'))
#     print(text_summarize(
#         "这个自然语言处理面对的面临的三大真实挑战，实际上是就现实挑战，实际挑战倒不这个这个词用的不一定。特别对，反正就现在我们面临的三大真实挑战，这个自然源处理的话被称作这个人工智能皇冠上的明珠。这个各位这个大佬都有很多论述，比如说这个图灵奖得主燕乐村就说深度学习的下一个前沿课题是自然语言理解等等，有很多这种论述。然后我个人也认为以自然语言为核心的语义理解是机器难以逾越的鸿沟。因为这个语言这一关如果机器搞明白了以后，这个我的话就机器真的成了精了，那时候我们人类真有危险了，真有危险，现在还还还还不是，所以这就进进一步彰显这个自然语言处理的这个困难，它已经成为制约人工智能取得更大突破的主要瓶颈之一，这是我的一个基本判断。那自然源处理的话，它实际上历史上有两大研究范式，就是理性主义、经验主义、经验主义我这就不讲了。实际上最近这最近这个经验主义的话，从90年代一直到现在也分了几波，最近这一两年从18年到现在，实际上我们大家都都熟悉的就大规模预训练语言模型，就是GD pGDP什么board，反正这一路到GDP three这一套东西，那这个 Gptc的话，今天其实大家很多都在谈，可见这个事对我们的这个震动，我的理解它实际上就是几个大就极大，即大规模模型及大规模数据集大规模计算，三个吉大光大还不够，加个急就导致出这个效果。这个给人感觉好像有量变引起质变的这个感觉，这个一方面是它的性能超乎我们的想象，一般我们想好像规模大不会引起这种变化。第二他也有一些这种一些科学现象，觉得有些奇怪，比如说这个这个这个这个这个就就是那个叫什么？Double descent。Uh那个 Dwd3的现象也很奇怪，这个跟我们一般机器学习那个道理不太一样，反正也所以它有量变引起质变的这么一些趋势。但实际上的话我们感觉经济three它这个有这种趋势，但是真是碰到一些这个涉及到质的问题，它是实际上还是解决的不太好，特别是一些深度的问题，比如说qd，你如果问他一些基本常识他没问题，但你如果去刁难他，你说搞烤箱和铅笔哪个更重，因为他没有知识，他就说铅笔比烤箱中这个就会乱讲，所以这个东西实际上到没到那个质变还不好说。这个用包括机器翻译，其实他做的也赶不上这个常规的这个办法，有些深度的像故事理解等等也不行，所以对某些任务可以，某些任务还是不行。那所以我们认为未来自然语言这个研究范式的走向就打个问号，他是不是就是沿着这个4个三个极大就往下走？我们认为可能不是可能不是不从从那个从人的这个角度，从机器角度两方面综合考虑的话，我们认为新的范式应应该是这个经验主义和理性主义的一种融合，我们叫大数据和复foo之双轮驱动，这个就现在大数据这一路都已经非常充分了，这个知识这块用的还是很不充分，所以这块我讲我们觉得要把知识给他加强，然后这两件事就走到过去的老路，完全靠知识也不写，应该是大数据和复制是两个双轮驱动。这种思路其实像燕乐村班给我他们亨城他们也有类似的这个说法，所以这也到不是我们新的这个这么这么一个思想，但这个说法我们我们认为是可能是我们这个方向将来未来几年这个要重点推介的一个方向，我们会沿着这个路子去做。那这里边实际上就面临三个挑战，三个挑战。总结对不对？大家这个批评第一个就是形式化知识系统，存在明显的构成，确实现在咱们都讲知识图谱，知识图谱规模有多大多大，包括什么第一批的Free base，这都说都几十亿个什么三元组，数以10亿计，但但这些知识我的理解实际上是大而不强，大而不强。这个你去看，我这个我一直举一个例子，你去看关羽，这个 Weekday里边关羽，它只有一些最简单的关于的这个这个这个属性描写，关于是是一个人，关羽是个将军，关于4个蜀国的这个关于他儿子是谁，孩子是谁是？身在哪，死在哪大概就这些知识。就所有关于关羽的什么那个什么过五关斩六将，什么这个这个三英战吕布等等的所有事通通没有统统没有，所以你这种这种知识库大，它其实它它覆盖面很窄。这个 Ibm的研究者曾经指出，比如沃森迪普克威伦的这个回答问题只有不到2%可以从dvp点Free based等知识库中直接匹配到，那就间接说明了我们这个观察所以这里面缺什么？缺关于动词关于那个动作的这个三元组，就比如说在哪斩了斩了华雄是？这个没有，这个实际上就是说主谓宾谓语就是展主语就是关公，谓语，就是那个那个宾语就是华雄。你把这个抽出来，这种事情其实你去看vkpd都有，但我们因为没有这个从句子分析这个三元组的这个这个主卫兵的能力，所以这件事做不成。好，所以这是一个系统性缺失，就是关于关于这个这个action、 Predicate这这些事实描述几乎是没有的，这是一个严重的缺失。另外一类就是说这个event和万的之间的关系，这个逻辑关系你比如说英国，比如说这个关于丢了荆州和关羽攻打樊城是什么关系？是？那应该说关于攻打防城是丢荆州的原因，是？这个东西没有，所以实际上是这是第二大块。所以现在的知识库三分天下缺了两分，这个它只是关于一些静态事实的描述这个比较充分，关于实体的一些静态描述比较充分，就关于这个事件基本描述几乎没有，关于事件与事件之间的关系，几乎没有。所以我叫三缺。其二这是一个大的问题，那这种是从哪来？现在的方法它基本上是为什么导致一下基本从Info boss来的vkdvkpd有Info，boss。简单事实是？那我说的那两分天下必须从文本中去找，尤其从VP的正文中去找，从这个从外部上去找，道理上这些东西都有，这个都有。我们现在是没有本事把它把他找来，这是第一个挑战。第二个挑战就是叫深层结构化语义分析，存在明显的性能不足，就要做刚才那个三分天下把两分给捞回来，依赖于什么？对句子的生存于我跟一个句子你应该有办法把主谓宾给我找出来。两个句子把它之间的关系找出来，这是最大的瓶颈。那这些年来应该说语义分析取得了这个长足的进步，你像考 Call know这2019英文的语义分析评测可以达到86%，这说明什么？我也真是试过一些工具，包括中文的包括英文的，说明这个也这个这个语义分析很难。但这些年已经有了很好的进步，就是他用来分析这些开放文本，有了一定的条件，所以这块的话我觉得现在条件好，我们可以去试了这事我们可以去试了。早些年不行，早10年不行，早10年这个分析效果太差，现在是有这个可能了。当然也面临很多挑战，比如说面向这个专用领域，像vkap的，我面向新闻会好一些，面向VIP的分析，其实VPN分析很难的，各种各种文本都有类型的，各种文各种这个领域的是？另外还有网上非规范的文本，这个分析精度会大幅下降，其实不止降5~10个点，可能降得更多，讲的更多。所以这里边就说我借那个知多元知识图谱就有一个就等于这又是个瓶颈，你这个问题不解决，你可能就对文本就没法进行分析。但同时的话你如果有了知识库，其实帮这个文本分析也会提供帮助，也会有所帮助。这个所以这两个实际上还存在一个鸡生蛋蛋生鸡的问题，这是第二个挑战。第三个挑战其实不是语言本身的，实际上是我们自然语言现在这个再往外走一走就是跨模态跨媒体。这个现在因为这个这个这个人的认知是多模态的多模态的，所以但其他的模态的理解除了有它自身的规律之外，我觉得复杂的语义场景识别都离不开文本。比如人脸识别是个简单的产品，虽然很复杂，但是它实际上就是识别这个人，对？这个对智能来说其实是一个相对简单的任务，只不过这种任务可能它超出了人的某种界限，人去认人，我记不住这么多人是？机器记得很容易，但是它本质上说可能是相对容易的任务。但是的话如果像视觉要去理解比较复杂的场景，就这个这个这个没有文本可能就非常吃力，开放我觉得根本做不到，只能在更宽的领域，所以这里边就有一个这个跨媒体跨语言融合，跨这个模态融合的问题。现在我认为现在这个系统我用的词叫形和意迷，反正现在有这个去甭管是训练这个训练这个这个文本和图像这个这个关联的这个波特什么之类的，反正基本上就两笔糊涂账，文本也是糊涂账，图像有糊涂账，两个糊涂蛋在一块，可能搞葱一点，因为量大，基本上形式合起来了，但意思他实际上抓不住这些，就是对了就对了，不对也就也就拉倒，大概是这么一个状况，我说的可能难听一点，所以叫形和印尼在这块确实是个挑战，就是说自然语言处理能力上去了，然后知识我也如果也上去了，那按理说对图像这文视频的理解就应该有帮助，所以这个我觉得也是一个挑战，现在的话你像文本的qa可以做到这个这这个可以做到93，93%，所以人类是89.5。那这个也不能太信，它实际上在一个数据集上做到这个真是到真的真的真的环境我看来还是不行。什么score地就那些东西都不太都不太行。这个宣传就说我超过人了，这玩意也得也得冷静的去看，不一定。但是但是在测试集上的话，你看文本问答还是做的不错的，视觉问答的话，如果是那个比较简单的做了76，如果是需要那个文本结合的，33.8一下就下来了。所以这块其实这是一个明显的一个短板，所以我认为的话，我们这个存在这三大挑战，那我们的新的这一个格局设想就是这三个东西要融合，我就不说了，就是大数据复制是跨模态，互相支持，互相促进。好，我最后总结一下就是我们这三大挑战，我我我希望我们这个通过几年的努力能不能实现不好说，但至少向这个方向去走，就是说制图谱重新的三缺，其二能不能到三三分皈依这个三国演义似的，三个东西都先齐活了，这个质量怎么样先不说。我们的这个这个先先往这个方向去努力，比如说文本这个 Vip的，我这100%的分析不行，我能不能从里面抽出30%的这个事实，三元组给放进来是？这30%进来会引起什么变化？这叫三分归一，那这样的话包括市里市里，那这样的话就构造一个新型的知识图谱，知识图谱内涵可能就会发生某种变化。再一个就是深层结构化，语义分析要从预行确指，现在我认为这我们叫预行确指，就是其实条件已经有就做的还行了，但是他真要走的时候又不太敢走，叫欲行却止，能不能我们叫且行且近，走走看看这个走一步进一步，这样总比不走好。再一个就跨模态语言理解，能不能从行合一迷到行合一合，这个要求高了，这个行合一合，这个这个高了，但是总是往里边有了前两条，第三条会不会做的好一点。所以这是我们说的三大挑战。"
#     ))
#     print(time.time() - beg)
